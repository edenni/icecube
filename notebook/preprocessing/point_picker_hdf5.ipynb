{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_valid_length:  6199.700247193777  ns\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icecube import constants\n",
    "\n",
    "# Data setting\n",
    "train_batch_id_first = 54\n",
    "train_batch_id_last = 95\n",
    "\n",
    "train_batch_ids = range(train_batch_id_first, train_batch_id_last + 1)\n",
    "\n",
    "max_pulse_count = 128\n",
    "bin_num = 8\n",
    "n_features = 9  # time, charge, aux, x, y, z, r_err, z_err, rank\n",
    "\n",
    "\n",
    "# sensor_geometry\n",
    "sensor_geometry_df = pd.read_csv(\"/media/eden/sandisk/projects/icecube/input/icecube/icecube-neutrinos-in-deep-ice/sensor_geometry.csv\")\n",
    "\n",
    "# counts\n",
    "doms_per_string = 60\n",
    "string_num = 86\n",
    "\n",
    "# index\n",
    "outer_long_strings = np.concatenate(\n",
    "    [np.arange(0, 25), np.arange(27, 34), np.arange(37, 44), np.arange(46, 78)]\n",
    ")\n",
    "inner_long_strings = np.array([25, 26, 34, 35, 36, 44, 45])\n",
    "inner_short_strings = np.array([78, 79, 80, 81, 82, 83, 84, 85])\n",
    "\n",
    "# known specs\n",
    "outer_xy_resolution = 125.0 / 2\n",
    "inner_xy_resolution = 70.0 / 2\n",
    "long_z_resolution = 17.0 / 2\n",
    "short_z_resolution = 7.0 / 2\n",
    "\n",
    "# evaluate error\n",
    "sensor_x = sensor_geometry_df.x\n",
    "sensor_y = sensor_geometry_df.y\n",
    "sensor_z = sensor_geometry_df.z\n",
    "sensor_r_err = np.ones(doms_per_string * string_num)\n",
    "sensor_z_err = np.ones(doms_per_string * string_num)\n",
    "\n",
    "for string_id in outer_long_strings:\n",
    "    sensor_r_err[\n",
    "        string_id * doms_per_string : (string_id + 1) * doms_per_string\n",
    "    ] *= outer_xy_resolution\n",
    "    \n",
    "for string_id in np.concatenate([inner_long_strings, inner_short_strings]):\n",
    "    sensor_r_err[\n",
    "        string_id * doms_per_string : (string_id + 1) * doms_per_string\n",
    "    ] *= inner_xy_resolution\n",
    "\n",
    "for string_id in outer_long_strings:\n",
    "    sensor_z_err[\n",
    "        string_id * doms_per_string : (string_id + 1) * doms_per_string\n",
    "    ] *= long_z_resolution\n",
    "\n",
    "for string_id in np.concatenate([inner_long_strings, inner_short_strings]):\n",
    "    for dom_id in range(doms_per_string):\n",
    "        z = sensor_z[string_id * doms_per_string + dom_id]\n",
    "        if (z < -156.0) or (z > 95.5 and z < 191.5):\n",
    "            sensor_z_err[\n",
    "                string_id * doms_per_string + dom_id\n",
    "            ] *= short_z_resolution\n",
    "\n",
    "# register\n",
    "sensor_geometry_df[\"r_err\"] = sensor_r_err\n",
    "sensor_geometry_df[\"z_err\"] = sensor_z_err\n",
    "\n",
    "# detector constants\n",
    "c_const = 0.299792458  # speed of light [m/ns]\n",
    "\n",
    "x_min = sensor_x.min()\n",
    "x_max = sensor_x.max()\n",
    "y_min = sensor_y.min()\n",
    "y_max = sensor_y.max()\n",
    "z_min = sensor_z.min()\n",
    "z_max = sensor_z.max()\n",
    "\n",
    "detector_length = np.sqrt((x_max - x_min)**2 + (y_max - y_min)**2 + (z_max - z_min)**2)\n",
    "t_valid_length = detector_length / c_const\n",
    "\n",
    "print(\"t_valid_length: \", t_valid_length, \" ns\")\n",
    "\n",
    "\n",
    "# read single event from batch_meta_df\n",
    "def read_event(event_idx, batch_meta_df, max_pulse_count, batch_df, train=True):\n",
    "    # read metadata\n",
    "    batch_id, first_pulse_index, last_pulse_index = batch_meta_df.iloc[event_idx][[\"batch_id\", \"first_pulse_index\", \"last_pulse_index\"]].astype(\"int\")\n",
    "\n",
    "    # read event\n",
    "    event_feature = batch_df[first_pulse_index:last_pulse_index + 1]\n",
    "    sensor_id = event_feature.sensor_id\n",
    "    event_id = event_feature.index.unique()\n",
    "    assert len(event_id) == 1\n",
    "    event_id = event_id[0]\n",
    "    \n",
    "    # merge features into single structured array\n",
    "    dtype = [\n",
    "        (\"time\", \"float16\"),\n",
    "        (\"charge\", \"float16\"),\n",
    "        (\"auxiliary\", \"float16\"),\n",
    "        (\"x\", \"float16\"),\n",
    "        (\"y\", \"float16\"),\n",
    "        (\"z\", \"float16\"),\n",
    "        (\"r_err\", \"float16\"),\n",
    "        (\"z_err\", \"float16\"),\n",
    "        (\"rank\", \"short\"),\n",
    "    ]\n",
    "    event_x = np.zeros(last_pulse_index - first_pulse_index + 1, dtype)\n",
    "\n",
    "    event_x[\"time\"] = event_feature.time.values - event_feature.time.min()\n",
    "    event_x[\"charge\"] = event_feature.charge.values\n",
    "    event_x[\"auxiliary\"] = event_feature.auxiliary.values\n",
    "\n",
    "    event_x[\"x\"] = sensor_geometry_df.x[sensor_id].values\n",
    "    event_x[\"y\"] = sensor_geometry_df.y[sensor_id].values\n",
    "    event_x[\"z\"] = sensor_geometry_df.z[sensor_id].values\n",
    "\n",
    "    event_x[\"r_err\"] = sensor_geometry_df.r_err[sensor_id].values\n",
    "    event_x[\"z_err\"] = sensor_geometry_df.z_err[sensor_id].values\n",
    "    \n",
    "    # For long event, pick-up\n",
    "    if len(event_x) > max_pulse_count:\n",
    "        # Find valid time window\n",
    "        t_peak = event_x[\"time\"][event_x[\"charge\"].argmax()]\n",
    "        t_valid_min = t_peak - t_valid_length\n",
    "        t_valid_max = t_peak + t_valid_length\n",
    "\n",
    "        t_valid = (event_x[\"time\"] > t_valid_min) * (event_x[\"time\"] < t_valid_max)\n",
    "\n",
    "        # rank\n",
    "        event_x[\"rank\"] = 2 * (1 - event_x[\"auxiliary\"]) + (t_valid)\n",
    "\n",
    "        # sort by rank and charge (important goes to backward)\n",
    "        event_x = np.sort(event_x, order=[\"rank\", \"charge\"])\n",
    "\n",
    "        # pick-up from backward\n",
    "        event_x = event_x[-max_pulse_count:]\n",
    "\n",
    "        # resort by time\n",
    "        event_x = np.sort(event_x, order=\"time\")\n",
    "\n",
    "    # for train data, give angles together\n",
    "    if train:\n",
    "        azimuth, zenith = batch_meta_df.iloc[event_idx][[\"azimuth\", \"zenith\"]].astype(\"float16\")\n",
    "        event_y = np.array([azimuth, zenith], dtype=\"float16\")\n",
    "        \n",
    "        return event_id, len(event_x), event_x, event_y\n",
    "    \n",
    "    # for test data, just give feature \n",
    "    else:\n",
    "        return event_id, len(event_x), event_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_format = '/media/eden/sandisk/projects/icecube/input/icecube/icecube-neutrinos-in-deep-ice/train/batch_{batch_id:d}.parquet'\n",
    "hdf5_file = \"/media/eden/sandisk/projects/icecube/input/icecube/hdf5/dataset.hdf5\"\n",
    "\n",
    "df_meta = pd.read_parquet(\"/media/eden/sandisk/projects/icecube/input/icecube/icecube-neutrinos-in-deep-ice/train_meta.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading batch  54 ...  DONE!\n",
      "Reading batch  55 ...  DONE!\n",
      "Reading batch  56 ...  DONE!\n",
      "Reading batch  57 ... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_208284/3569983981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{event_id} already saved. Skip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mgroup_train_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mgroup_train_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/icecube/lib/python3.7/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mdsid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_new_dset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/icecube/lib/python3.7/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0msid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mdset_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import gc\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "with h5py.File(hdf5_file, 'w') as h5:\n",
    "    group_train_data = h5.create_group(\"X_train\")\n",
    "    group_train_label = h5.create_group(\"y_train\")\n",
    "\n",
    "    for batch_id in train_batch_ids:\n",
    "        batch_train = group_train_data.create_group(str(batch_id))\n",
    "        batch_label = group_train_label.create_group(str(batch_id))\n",
    "\n",
    "        print(\"Reading batch \", batch_id, \"... \", end=\"\")\n",
    "        # get batch meta data and data\n",
    "        batch_meta_df = df_meta[df_meta.batch_id == batch_id]\n",
    "        batch_df = pd.read_parquet(train_format.format(batch_id=batch_id))\n",
    "\n",
    "        # register pulses\n",
    "        batch_x = np.zeros((max_pulse_count, n_features), dtype=\"float16\")\n",
    "        batch_y = np.zeros((2), dtype=\"float16\")\n",
    "        \n",
    "        batch_x[:, 2] = -1\n",
    "\n",
    "        def read_event_local(event_idx):\n",
    "            return read_event(event_idx, batch_meta_df, max_pulse_count, batch_df, train=True)\n",
    "\n",
    "        X = {}\n",
    "        y = {}\n",
    "        # scan events\n",
    "        iterator = range(len(batch_meta_df))\n",
    "        with multiprocessing.Pool() as pool:\n",
    "            for event_id, pulse_count, event_x, event_y in pool.map(read_event_local, iterator):\n",
    "                batch_x[:pulse_count, 0] = event_x[\"time\"]\n",
    "                batch_x[:pulse_count, 1] = event_x[\"charge\"]\n",
    "                batch_x[:pulse_count, 2] = event_x[\"auxiliary\"]\n",
    "                batch_x[:pulse_count, 3] = event_x[\"x\"]\n",
    "                batch_x[:pulse_count, 4] = event_x[\"y\"]\n",
    "                batch_x[:pulse_count, 5] = event_x[\"z\"]\n",
    "                batch_x[:pulse_count, 6] = event_x[\"r_err\"]\n",
    "                batch_x[:pulse_count, 7] = event_x[\"z_err\"]\n",
    "                batch_x[:pulse_count, 8] = event_x[\"rank\"]\n",
    "\n",
    "                X[str(event_id)] = batch_x\n",
    "                y[str(event_id)] = event_y\n",
    "\n",
    "        # Write hdf5\n",
    "        for event_id in X:\n",
    "            batch_train.create_dataset(event_id, data=X[event_id])\n",
    "            batch_train.create_dataset(event_id, data=y[event_id])\n",
    "\n",
    "        del batch_meta_df, batch_df, X, y, batch_x, batch_y\n",
    "        gc.collect()\n",
    "\n",
    "        # save\n",
    "        print(\" DONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icecube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1f5c8a913c9cf5c53f678b20c057b673b014e2f0c3fa6d6f65aab94461faf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
