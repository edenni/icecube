{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/seungmoklee/3-lstms-with-data-picking-and-shifting#Set-Detector-Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from icecube import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data setting\n",
    "train_batch_id_first = 81\n",
    "train_batch_id_last = 100\n",
    "\n",
    "train_batch_ids = range(train_batch_id_first, train_batch_id_last + 1)\n",
    "\n",
    "max_pulse_count = # 128\n",
    "bin_num = 8\n",
    "n_features = 6 #9  # time, charge, aux, x, y, z, r_err, z_err, rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sensor_geometry\n",
    "sensor_geometry_df = pd.read_csv(\"/media/eden/sandisk/projects/icecube/input/icecube/icecube-neutrinos-in-deep-ice/sensor_geometry.csv\")\n",
    "\n",
    "# counts\n",
    "doms_per_string = 60\n",
    "string_num = 86\n",
    "\n",
    "# index\n",
    "outer_long_strings = np.concatenate(\n",
    "    [np.arange(0, 25), np.arange(27, 34), np.arange(37, 44), np.arange(46, 78)]\n",
    ")\n",
    "inner_long_strings = np.array([25, 26, 34, 35, 36, 44, 45])\n",
    "inner_short_strings = np.array([78, 79, 80, 81, 82, 83, 84, 85])\n",
    "\n",
    "# known specs\n",
    "outer_xy_resolution = 125.0 / 2\n",
    "inner_xy_resolution = 70.0 / 2\n",
    "long_z_resolution = 17.0 / 2\n",
    "short_z_resolution = 7.0 / 2\n",
    "\n",
    "# evaluate error\n",
    "sensor_x = sensor_geometry_df.x\n",
    "sensor_y = sensor_geometry_df.y\n",
    "sensor_z = sensor_geometry_df.z\n",
    "sensor_r_err = np.ones(doms_per_string * string_num)\n",
    "sensor_z_err = np.ones(doms_per_string * string_num)\n",
    "\n",
    "for string_id in outer_long_strings:\n",
    "    sensor_r_err[\n",
    "        string_id * doms_per_string : (string_id + 1) * doms_per_string\n",
    "    ] *= outer_xy_resolution\n",
    "    \n",
    "for string_id in np.concatenate([inner_long_strings, inner_short_strings]):\n",
    "    sensor_r_err[\n",
    "        string_id * doms_per_string : (string_id + 1) * doms_per_string\n",
    "    ] *= inner_xy_resolution\n",
    "\n",
    "for string_id in outer_long_strings:\n",
    "    sensor_z_err[\n",
    "        string_id * doms_per_string : (string_id + 1) * doms_per_string\n",
    "    ] *= long_z_resolution\n",
    "\n",
    "for string_id in np.concatenate([inner_long_strings, inner_short_strings]):\n",
    "    for dom_id in range(doms_per_string):\n",
    "        z = sensor_z[string_id * doms_per_string + dom_id]\n",
    "        if (z < -156.0) or (z > 95.5 and z < 191.5):\n",
    "            sensor_z_err[\n",
    "                string_id * doms_per_string + dom_id\n",
    "            ] *= short_z_resolution\n",
    "\n",
    "# register\n",
    "sensor_geometry_df[\"r_err\"] = sensor_r_err\n",
    "sensor_geometry_df[\"z_err\"] = sensor_z_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_valid_length:  6199.700247193777  ns\n"
     ]
    }
   ],
   "source": [
    "# detector constants\n",
    "c_const = 0.299792458  # speed of light [m/ns]\n",
    "\n",
    "x_min = sensor_x.min()\n",
    "x_max = sensor_x.max()\n",
    "y_min = sensor_y.min()\n",
    "y_max = sensor_y.max()\n",
    "z_min = sensor_z.min()\n",
    "z_max = sensor_z.max()\n",
    "\n",
    "detector_length = np.sqrt((x_max - x_min)**2 + (y_max - y_min)**2 + (z_max - z_min)**2)\n",
    "t_valid_length = detector_length / c_const\n",
    "\n",
    "print(\"t_valid_length: \", t_valid_length, \" ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read single event from batch_meta_df\n",
    "def read_event(event_idx, batch_meta_df, max_pulse_count, batch_df, train=True):\n",
    "    # read metadata\n",
    "    batch_id, first_pulse_index, last_pulse_index = batch_meta_df.iloc[event_idx][[\"batch_id\", \"first_pulse_index\", \"last_pulse_index\"]].astype(\"int\")\n",
    "\n",
    "    # read event\n",
    "    event_feature = batch_df[first_pulse_index:last_pulse_index + 1]\n",
    "    sensor_id = event_feature.sensor_id\n",
    "    \n",
    "    # merge features into single structured array\n",
    "    dtype = [\n",
    "        (\"time\", \"float16\"),\n",
    "        (\"charge\", \"float16\"),\n",
    "        (\"auxiliary\", \"float16\"),\n",
    "        (\"x\", \"float16\"),\n",
    "        (\"y\", \"float16\"),\n",
    "        (\"z\", \"float16\"),\n",
    "        (\"r_err\", \"float16\"),\n",
    "        (\"z_err\", \"float16\"),\n",
    "        (\"rank\", \"short\"),\n",
    "    ]\n",
    "    event_x = np.zeros(last_pulse_index - first_pulse_index + 1, dtype)\n",
    "\n",
    "    event_x[\"time\"] = event_feature[\"time\"].values - event_feature[\"time\"].min()\n",
    "    event_x[\"charge\"] = event_feature.charge.values\n",
    "    event_x[\"auxiliary\"] = event_feature.auxiliary.values\n",
    "\n",
    "    event_x[\"x\"] = sensor_geometry_df.x[sensor_id].values\n",
    "    event_x[\"y\"] = sensor_geometry_df.y[sensor_id].values\n",
    "    event_x[\"z\"] = sensor_geometry_df.z[sensor_id].values\n",
    "\n",
    "    event_x[\"r_err\"] = sensor_geometry_df.r_err[sensor_id].values\n",
    "    event_x[\"z_err\"] = sensor_geometry_df.z_err[sensor_id].values\n",
    "    \n",
    "    # For long event, pick-up\n",
    "    if len(event_x) > max_pulse_count:\n",
    "        # Find valid time window\n",
    "        t_peak = event_x[\"time\"][event_x[\"charge\"].argmax()]\n",
    "        t_valid_min = t_peak - t_valid_length\n",
    "        t_valid_max = t_peak + t_valid_length\n",
    "\n",
    "        t_valid = (event_x[\"time\"] > t_valid_min) * (event_x[\"time\"] < t_valid_max)\n",
    "\n",
    "        # rank\n",
    "        event_x[\"rank\"] = 2 * (1 - event_x[\"auxiliary\"]) + (t_valid)\n",
    "\n",
    "        # sort by rank and charge (important goes to backward)\n",
    "        event_x = np.sort(event_x, order=[\"rank\", \"charge\"])\n",
    "\n",
    "        # pick-up from backward\n",
    "        event_x = event_x[-max_pulse_count:]\n",
    "\n",
    "        # resort by time\n",
    "        event_x = np.sort(event_x, order=\"time\")\n",
    "\n",
    "    # for train data, give angles together\n",
    "    if train:\n",
    "        azimuth, zenith = batch_meta_df.iloc[event_idx][[\"azimuth\", \"zenith\"]].astype(\"float16\")\n",
    "        event_y = np.array([azimuth, zenith], dtype=\"float16\")\n",
    "        \n",
    "        return event_idx, len(event_x), event_x, event_y\n",
    "    \n",
    "    # for test data, just give feature \n",
    "    else:\n",
    "        return event_idx, len(event_x), event_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_format = '/media/eden/sandisk/projects/icecube/input/icecube/icecube-neutrinos-in-deep-ice/train/batch_{batch_id:d}.parquet'\n",
    "point_picker_format = \"../../input/preprocessed/pointpicker_mpc128_n9_batch_{batch_id:d}.npz\"\n",
    "\n",
    "df_meta = pd.read_parquet(\"/media/eden/sandisk/projects/icecube/input/icecube/icecube-neutrinos-in-deep-ice/train_meta.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading batch  81 DONE! Saving...\n",
      "Reading batch  82 DONE! Saving...\n",
      "Reading batch  83 DONE! Saving...\n",
      "Reading batch  84 DONE! Saving...\n",
      "Reading batch  85 DONE! Saving...\n",
      "Reading batch  86 DONE! Saving...\n",
      "Reading batch  87 DONE! Saving...\n",
      "Reading batch  88 DONE! Saving...\n",
      "Reading batch  89 DONE! Saving...\n",
      "Reading batch  90 DONE! Saving...\n",
      "Reading batch  91 DONE! Saving...\n",
      "Reading batch  92 DONE! Saving...\n",
      "Reading batch  93 DONE! Saving...\n",
      "Reading batch  94 DONE! Saving...\n",
      "Reading batch  95 DONE! Saving...\n",
      "Reading batch  96 DONE! Saving...\n",
      "Reading batch  97 DONE! Saving...\n",
      "Reading batch  98 DONE! Saving...\n",
      "Reading batch  99 DONE! Saving...\n",
      "Reading batch  100 DONE! Saving...\n",
      "CPU times: user 1min 27s, sys: 48.6 s, total: 2min 16s\n",
      "Wall time: 14min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "\n",
    "for batch_id in train_batch_ids:\n",
    "    print(\"Reading batch \", batch_id, end=\"\")\n",
    "    # get batch meta data and data\n",
    "    batch_meta_df = df_meta[df_meta.batch_id == batch_id]\n",
    "    batch_df = pd.read_parquet(train_format.format(batch_id=batch_id))\n",
    "\n",
    "    # register pulses\n",
    "    batch_x = np.zeros((len(batch_meta_df), max_pulse_count, n_features), dtype=\"float16\")\n",
    "    batch_y = np.zeros((len(batch_meta_df), 2), dtype=\"float16\")\n",
    "    \n",
    "    batch_x[:, :, 2] = -1\n",
    "    \n",
    "\n",
    "    def read_event_local(event_idx):\n",
    "        return read_event(event_idx, batch_meta_df, max_pulse_count, batch_df, train=True)\n",
    "\n",
    "    \n",
    "    # scan events\n",
    "    iterator = range(len(batch_meta_df))\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        for event_idx, pulse_count, event_x, event_y in pool.map(read_event_local, iterator):\n",
    "            batch_x[event_idx, :pulse_count, 0] = event_x[\"time\"]\n",
    "            batch_x[event_idx, :pulse_count, 1] = event_x[\"charge\"]\n",
    "            batch_x[event_idx, :pulse_count, 2] = event_x[\"auxiliary\"]\n",
    "            batch_x[event_idx, :pulse_count, 3] = event_x[\"x\"]\n",
    "            batch_x[event_idx, :pulse_count, 4] = event_x[\"y\"]\n",
    "            batch_x[event_idx, :pulse_count, 5] = event_x[\"z\"]\n",
    "            batch_x[event_idx, :pulse_count, 6] = event_x[\"r_err\"]\n",
    "            batch_x[event_idx, :pulse_count, 7] = event_x[\"z_err\"]\n",
    "            batch_x[event_idx, :pulse_count, 8] = event_x[\"rank\"]\n",
    "\n",
    "            batch_y[event_idx] = event_y\n",
    "\n",
    "    del batch_meta_df, batch_df\n",
    "    \n",
    "    # save\n",
    "    print(\" DONE! Saving...\")\n",
    "    np.savez(point_picker_format.format(batch_id=batch_id), x=batch_x, y=batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = [\n",
    "    (\"time\", \"float16\"),\n",
    "    (\"charge\", \"float16\"),\n",
    "    (\"auxiliary\", \"float16\"),\n",
    "    (\"x\", \"float16\"),\n",
    "    (\"y\", \"float16\"),\n",
    "    (\"z\", \"float16\"),\n",
    "    (\"r_err\", \"float16\"),\n",
    "    (\"z_err\", \"float16\"),\n",
    "    (\"rank\", \"short\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d1f5c8a913c9cf5c53f678b20c057b673b014e2f0c3fa6d6f65aab94461faf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
