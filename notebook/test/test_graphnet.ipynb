{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.profiler import SimpleProfiler\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import (\n",
    "    DirectionReconstructionWithKappa,\n",
    ")\n",
    "from graphnet.training.callbacks import PiecewiseLinearLR, ProgressBar\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss\n",
    "from graphnet.training.utils import make_dataloader\n",
    "from graphnet.utilities.logging import get_logger\n",
    "from icecube.constants import *\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "PULSEMAP = \"pulse_table\"\n",
    "DATABASE_PATH = database_dir / \"batch_51_100.db\"\n",
    "# DATABASE_PATH = \"/media/eden/sandisk/projects/icecube/input/sqlite/batch_1.db\"\n",
    "PULSE_THRESHOLD = 400\n",
    "SEED = 42\n",
    "\n",
    "# Training configs\n",
    "MAX_EPOCHS = 100\n",
    "LR = 5e-4\n",
    "MOMENTUM = 0.9\n",
    "BS = 256\n",
    "ES = 10\n",
    "NUM_FOLDS = 5\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Paths\n",
    "FOLD_PATH = input_dir / \"folds\"\n",
    "COUNT_PATH = FOLD_PATH / \"batch51_100_counts.csv\"\n",
    "CV_PATH = FOLD_PATH / f\"batch51_100_cv_max_{PULSE_THRESHOLD}_pulses.csv\"\n",
    "WANDB_DIR = log_dir\n",
    "PROJECT_NAME = \"icecube\"\n",
    "GROUP_NAME = \"pretrain_sub_5_batch_51_100_large_resume\"\n",
    "\n",
    "CREATE_FOLDS = False\n",
    "\n",
    "\n",
    "def make_selection(\n",
    "    df: pd.DataFrame, num_folds: int = 5, pulse_threshold: int = 200\n",
    ") -> None:\n",
    "    \"\"\"Creates a validation and training selection (20 - 80). All events in both selections satisfies n_pulses <= 200 by default.\"\"\"\n",
    "    n_events = np.arange(0, len(df), 1)\n",
    "    df[\"fold\"] = 0\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "    for i, (_, val_idx) in enumerate(kf.split(n_events)):\n",
    "        df.loc[val_idx, \"fold\"] = i\n",
    "\n",
    "    # Remove events with large pulses from training and validation sample (memory)\n",
    "    df[\"fold\"][df[\"n_pulses\"] > pulse_threshold] = -1\n",
    "\n",
    "    df.to_csv(CV_PATH)\n",
    "    return\n",
    "\n",
    "\n",
    "def get_number_of_pulses(db: Path, event_id: int, pulsemap: str) -> int:\n",
    "    with sqlite3.connect(str(db)) as con:\n",
    "        query = f\"select event_id from {pulsemap} where event_id = {event_id} limit 20000\"\n",
    "        data = con.execute(query).fetchall()\n",
    "    return len(data)\n",
    "\n",
    "\n",
    "def count_pulses(database: Path, pulsemap: str) -> pd.DataFrame:\n",
    "    \"\"\"Will count the number of pulses in each event and return a single dataframe that contains counts for each event_id.\"\"\"\n",
    "    with sqlite3.connect(str(database)) as con:\n",
    "        query = \"select event_id from meta_table\"\n",
    "        events = pd.read_sql(query, con)\n",
    "    counts = {\"event_id\": [], \"n_pulses\": []}\n",
    "\n",
    "    for event_id in tqdm(events[\"event_id\"]):\n",
    "        a = get_number_of_pulses(database, event_id, pulsemap)\n",
    "        counts[\"event_id\"].append(event_id)\n",
    "        counts[\"n_pulses\"].append(a)\n",
    "\n",
    "    df = pd.DataFrame(counts)\n",
    "    df.to_csv(COUNT_PATH)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    config: Dict[str, Any], train_dataloader: Any = None\n",
    ") -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    "        dynedge_layer_sizes=config.get(\"layer_sizes\"),\n",
    "    )\n",
    "    if config.get(\"activation\"):\n",
    "        gnn._activation = config[\"activation\"]()\n",
    "\n",
    "    if config[\"target\"] == \"direction\":\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [\n",
    "            config[\"target\"] + \"_x\",\n",
    "            config[\"target\"] + \"_y\",\n",
    "            config[\"target\"] + \"_z\",\n",
    "            config[\"target\"] + \"_kappa\",\n",
    "        ]\n",
    "        additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=SGD,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\": LR,\n",
    "            \"momentum\": MOMENTUM,\n",
    "            \"nesterov\": True,\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_pretrained_model(\n",
    "    config: Dict[str, Any],\n",
    "    state_dict_path: str = \"/kaggle/input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth\",\n",
    ") -> StandardModel:\n",
    "    # train_dataloader, _ = make_dataloaders(config=config)\n",
    "    model = build_model(config=config, train_dataloader=None)\n",
    "    # model._inference_trainer = Trainer(config['fit'])\n",
    "    try:\n",
    "        model.load_state_dict(state_dict_path)\n",
    "    except:\n",
    "        state_dict = torch.load(state_dict_path)[\"state_dict\"]\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    model.prediction_columns = [\n",
    "        config[\"target\"] + \"_x\",\n",
    "        config[\"target\"] + \"_y\",\n",
    "        config[\"target\"] + \"_z\",\n",
    "        config[\"target\"] + \"_kappa\",\n",
    "    ]\n",
    "    model.additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any], fold: int = 0) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "    df_cv = pd.read_csv(CV_PATH)\n",
    "\n",
    "    val_idx = (\n",
    "        df_cv[df_cv[\"fold\"] == fold][config[\"index_column\"]].ravel().tolist()\n",
    "    )\n",
    "    train_idx = (\n",
    "        df_cv[~df_cv[\"fold\"].isin([-1, fold])][config[\"index_column\"]]\n",
    "        .ravel()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    train_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=train_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    validate_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=val_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "\n",
    "def convert_to_3d(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    df[\"true_x\"] = np.cos(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_y\"] = np.sin(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_z\"] = np.cos(df[\"zenith\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_angular_error(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcualtes the opening angle (angular error) between true and reconstructed direction vectors\"\"\"\n",
    "    df[\"angular_error\"] = np.arccos(\n",
    "        df[\"true_x\"] * df[\"direction_x\"]\n",
    "        + df[\"true_y\"] * df[\"direction_y\"]\n",
    "        + df[\"true_z\"] * df[\"direction_z\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def inference(model, config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Applies model to the database specified in config['inference_database_path'] and saves results to disk.\"\"\"\n",
    "    # Make Dataloader\n",
    "    test_dataloader = make_dataloader(\n",
    "        db=config[\"inference_database_path\"],\n",
    "        selection=None,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=config[\"features\"],\n",
    "        truth=config[\"truth\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "        labels=None,  # Cannot make labels in test data\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    # Get predictions\n",
    "    results = model.predict_as_dataframe(\n",
    "        gpus=[0],\n",
    "        dataloader=test_dataloader,\n",
    "        prediction_columns=model.prediction_columns,\n",
    "        additional_attributes=model.additional_attributes,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "def prepare_dataframe(\n",
    "    df, angle_post_fix=\"_reco\", vec_post_fix=\"\"\n",
    ") -> pd.DataFrame:\n",
    "    r = np.sqrt(\n",
    "        df[\"direction_x\" + vec_post_fix] ** 2\n",
    "        + df[\"direction_y\" + vec_post_fix] ** 2\n",
    "        + df[\"direction_z\" + vec_post_fix] ** 2\n",
    "    )\n",
    "    df[\"zenith\" + angle_post_fix] = np.arccos(\n",
    "        df[\"direction_z\" + vec_post_fix] / r\n",
    "    )\n",
    "    df[\"azimuth\" + angle_post_fix] = np.arctan2(\n",
    "        df[\"direction_y\" + vec_post_fix], df[\"direction_x\" + vec_post_fix]\n",
    "    )  # np.sign(results['true_y'])*np.arccos((results['true_x'])/(np.sqrt(results['true_x']**2 + results['true_y']**2)))\n",
    "    df[\"azimuth\" + angle_post_fix][df[\"azimuth\" + angle_post_fix] < 0] = (\n",
    "        df[\"azimuth\" + angle_post_fix][df[\"azimuth\" + angle_post_fix] < 0]\n",
    "        + 2 * np.pi\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def angular_dist_score(\n",
    "    az_true: np.ndarray,\n",
    "    zen_true: np.ndarray,\n",
    "    az_pred: np.ndarray,\n",
    "    zen_pred: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the MAE of the angular distance between two directions.\n",
    "    The two vectors are first converted to cartesian unit vectors,\n",
    "    and then their scalar product is computed, which is equal to\n",
    "    the cosine of the angle between the two vectors. The inverse\n",
    "    cosine (arccos) thereof is then the angle between the two input vectors\n",
    "\n",
    "    Args:\n",
    "        az_true  (np.ndarray): true azimuth value(s) in radian\n",
    "        zen_true (np.ndarray): true zenith value(s) in radian\n",
    "        az_pred  (np.ndarray): predicted azimuth value(s) in radian\n",
    "        zen_pred (np.ndarray): predicted zenith value(s) in radian\n",
    "\n",
    "    Returns:\n",
    "        float: mean over the angular distance(s) in radian\n",
    "    \"\"\"\n",
    "\n",
    "    if not (\n",
    "        np.all(np.isfinite(az_true))\n",
    "        and np.all(np.isfinite(zen_true))\n",
    "        and np.all(np.isfinite(az_pred))\n",
    "        and np.all(np.isfinite(zen_pred))\n",
    "    ):\n",
    "        raise ValueError(\"All arguments must be finite\")\n",
    "\n",
    "    # Pre-compute all sine and cosine values\n",
    "    sa1 = np.sin(az_true)\n",
    "    ca1 = np.cos(az_true)\n",
    "    sz1 = np.sin(zen_true)\n",
    "    cz1 = np.cos(zen_true)\n",
    "\n",
    "    sa2 = np.sin(az_pred)\n",
    "    ca2 = np.cos(az_pred)\n",
    "    sz2 = np.sin(zen_pred)\n",
    "    cz2 = np.cos(zen_pred)\n",
    "\n",
    "    # Scalar product of the two cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n",
    "    scalar_prod = sz1 * sz2 * (ca1 * ca2 + sa1 * sa2) + (cz1 * cz2)\n",
    "\n",
    "    # Scalar product of two unit vectors is always between -1 and 1, this is against nummerical instability\n",
    "    # That might otherwise occure from the finite precision of the sine and cosine functions\n",
    "    scalar_prod = np.clip(scalar_prod, -1, 1)\n",
    "\n",
    "    # Convert back to an angle (in radian)\n",
    "    return np.average(np.abs(np.arccos(scalar_prod)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config1 = {\n",
    "    \"inference_database_path\": \"../../input/icecube/sqlite/test_batch_641_660.db\",\n",
    "    \"path\": str(DATABASE_PATH),\n",
    "    \"pulsemap\": \"pulse_table\",\n",
    "    \"truth_table\": \"meta_table\",\n",
    "    \"features\": FEATURES.KAGGLE,\n",
    "    \"truth\": TRUTH.KAGGLE,\n",
    "    \"index_column\": \"event_id\",\n",
    "    \"batch_size\": BS,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"target\": \"direction\",\n",
    "    \"run_name_tag\": \"batch_1_50\",\n",
    "    \"early_stopping_patience\": ES,\n",
    "    \"fit\": {\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"gpus\": [0],\n",
    "        \"distribution_strategy\": None,\n",
    "        \"limit_train_batches\": 0.1,  # debug\n",
    "        \"limit_val_batches\": 0.1,\n",
    "    },\n",
    "    \"base_dir\": \"training\",\n",
    "    \"wandb\": {\n",
    "        \"project\": PROJECT_NAME,\n",
    "        \"group\": GROUP_NAME,\n",
    "    },\n",
    "    \"lr\": LR,\n",
    "}\n",
    "\n",
    "\n",
    "config2 = {\n",
    "    \"inference_database_path\": \"../../input/icecube/sqlite/test_batch_641_660.db\",\n",
    "    \"path\": str(DATABASE_PATH),\n",
    "    \"pulsemap\": \"pulse_table\",\n",
    "    \"truth_table\": \"meta_table\",\n",
    "    \"features\": FEATURES.KAGGLE,\n",
    "    \"truth\": TRUTH.KAGGLE,\n",
    "    \"index_column\": \"event_id\",\n",
    "    \"batch_size\": BS,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"target\": \"direction\",\n",
    "    \"run_name_tag\": \"batch_1_50\",\n",
    "    \"early_stopping_patience\": ES,\n",
    "    \"fit\": {\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"gpus\": [0],\n",
    "        \"distribution_strategy\": None,\n",
    "        \"limit_train_batches\": 0.1,  # debug\n",
    "        \"limit_val_batches\": 0.1,\n",
    "    },\n",
    "    \"base_dir\": \"training\",\n",
    "    \"wandb\": {\n",
    "        \"project\": PROJECT_NAME,\n",
    "        \"group\": GROUP_NAME,\n",
    "    },\n",
    "    \"lr\": LR,\n",
    "    \"activation\": torch.nn.Mish,\n",
    "    \"layer_sizes\": \n",
    "        [\n",
    "                    (\n",
    "                        128,\n",
    "                        256,\n",
    "                    ),\n",
    "                    (\n",
    "                        336,\n",
    "                        256,\n",
    "                    ),\n",
    "                    (\n",
    "                        336,\n",
    "                        256,\n",
    "                        256,\n",
    "                    ),\n",
    "                    (\n",
    "                        336,\n",
    "                        256,\n",
    "                        256,\n",
    "                    ),\n",
    "                ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt1 = \"../../models/graphnet/batch1_50/state_dict.pth\"\n",
    "ckpt2 = \"/media/eden/sandisk/projects/icecube/logs/icecube/eqk4nmi1/checkpoints/graphnet-val/mae=1.0763-epoch=21.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e6dc2a1ab54033b0dd4aa301b789be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f91b88edcb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "\n",
      "AssertionError: can only test a child processException ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f91b88edcb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f91b88edcb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f91b88edcb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f91b88edcb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f91b88edcb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f91b88edcb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f91b88edcb0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1449, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_448011/2022909856.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_448011/276742730.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(model, config)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mprediction_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_columns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0madditional_attributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     )\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/eden/sandisk/projects/icecube/src/graphnet/src/graphnet/models/model.py\u001b[0m in \u001b[0;36mpredict_as_dataframe\u001b[0;34m(self, dataloader, prediction_columns, node_level, additional_attributes, index_column, gpus, distribution_strategy)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         )\n\u001b[1;32m    186\u001b[0m         predictions = (\n",
      "\u001b[0;32m/media/eden/sandisk/projects/icecube/src/graphnet/src/graphnet/models/standard_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, dataloader, gpus, distribution_strategy)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         )\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/eden/sandisk/projects/icecube/src/graphnet/src/graphnet/models/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, dataloader, gpus, distribution_strategy)\u001b[0m\n\u001b[1;32m    134\u001b[0m             )\n\u001b[1;32m    135\u001b[0m         \u001b[0mpredictions_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Got no predictions\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mnb_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "model1 = load_pretrained_model(config1, state_dict_path=ckpt1)\n",
    "result1 = inference(model1, config1)\n",
    "\n",
    "model2 = load_pretrained_model(config2, state_dict_path=ckpt2)\n",
    "result2 = inference(model2, config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_size = 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/pandas/core/arraylike.py:364: RuntimeWarning: invalid value encountered in arccos\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of model 1:  1.02621\n",
      "MAE of model 2:  1.01082\n"
     ]
    }
   ],
   "source": [
    "results1 = calculate_angular_error(convert_to_3d(result1))\n",
    "results2 = calculate_angular_error(convert_to_3d(result2))\n",
    "\n",
    "print(\"MAE of model 1: \", np.round(results1.iloc[-test_size:][\"angular_error\"].mean(), 5))\n",
    "print(\"MAE of model 2: \", np.round(results2.iloc[-test_size:][\"angular_error\"].mean(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0260052559287693 1.0104395380336963\n"
     ]
    }
   ],
   "source": [
    "results1 = prepare_dataframe(results1)\n",
    "results2 = prepare_dataframe(results2)\n",
    "\n",
    "print(angular_dist_score(results1[\"azimuth\"], results1[\"zenith\"], results1[\"azimuth_reco\"], results1[\"zenith_reco\"]),\n",
    "angular_dist_score(results2[\"azimuth\"], results2[\"zenith\"], results2[\"azimuth_reco\"], results2[\"zenith_reco\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_fea = [\"direction_x\", \"direction_y\", \"direction_z\"]\n",
    "col_tar = [\"true_x\", \"true_y\", \"true_z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "             max_delta_step=0, max_depth=4, max_leaves=0, min_child_weight=1,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, ...)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate([results1[col_fea], results2[col_fea]], axis=1)\n",
    "y = results1[col_tar].values\n",
    "\n",
    "X_train = X[:-test_size]\n",
    "y_train = y[:-test_size]\n",
    "X_test = X[-test_size:]\n",
    "y_test = y[-test_size:]\n",
    "\n",
    "rgr = xgb.XGBRegressor(n_estimators=100, max_depth=4)\n",
    "rgr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of ensemble:  1.02\n"
     ]
    }
   ],
   "source": [
    "preds = rgr.predict(X_test)\n",
    "preds = preds / np.linalg.norm(preds, axis=1).reshape(-1, 1)\n",
    "\n",
    "ensem_results = results1.iloc[-test_size:].copy()\n",
    "ensem_results[col_fea] = preds\n",
    "ensem_results = calculate_angular_error(convert_to_3d(ensem_results))\n",
    "print(\"MAE of ensemble: \", np.round(ensem_results[\"angular_error\"].mean(),2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0140570780150844"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_results = results1.copy()\n",
    "weighted_results[col_fea] = results1[col_fea].values * 0.7 + results2[col_fea].values * 0.3\n",
    "weighted_results[col_fea] /= np.linalg.norm(weighted_results[col_fea], axis=1).reshape(-1, 1)\n",
    "weighted_results = calculate_angular_error(convert_to_3d(weighted_results))\n",
    "\n",
    "# print(\"MAE of ensemble: \", np.round(weighted_results.iloc[-test_size:][\"angular_error\"].mean(),5))\n",
    "weighted_results = prepare_dataframe(weighted_results)\n",
    "angular_dist_score(weighted_results[\"azimuth\"], weighted_results[\"zenith\"], weighted_results[\"azimuth_reco\"], weighted_results[\"zenith_reco\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d1f5c8a913c9cf5c53f678b20c057b673b014e2f0c3fa6d6f65aab94461faf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
