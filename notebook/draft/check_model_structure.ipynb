{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-19 02:42:43 - get_logger - Writing log to \u001b[1m/media/eden/sandisk/projects/icecube/logs/graphnet_20230219-024243.log\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-19 02:42:52 - warn_once - `icecube` not available. Some functionality may be missing.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sqlite3\n",
    "from typing import List, Optional, Dict, Any\n",
    "from pathlib import Path\n",
    "prj_path = Path(\"/media/eden/sandisk/projects/icecube/\")\n",
    "sys.path.append(str(prj_path))\n",
    "\n",
    "from src.constants import *\n",
    "from graphnet.utilities.logging import get_logger\n",
    "\n",
    "logger = get_logger(log_folder=log_dir)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lion_pytorch import Lion\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim.adam import Adam\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import (\n",
    "    DirectionReconstructionWithKappa,\n",
    "    ZenithReconstructionWithKappa,\n",
    "    AzimuthReconstructionWithKappa,\n",
    ")\n",
    "from graphnet.training.callbacks import ProgressBar, PiecewiseLinearLR\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss, VonMisesFisher2DLoss\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.utils import make_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PULSEMAP = \"pulse_table\"\n",
    "DATABASE_PATH = database_dir / \"batch_51_100.db\"\n",
    "# DATABASE_PATH = \"/media/eden/sandisk/projects/icecube/input/sqlite/batch_1.db\"\n",
    "PULSE_THRESHOLD = 400\n",
    "SEED = 42\n",
    "\n",
    "# Training configs\n",
    "MAX_EPOCHS = 100\n",
    "LR = 1e-3\n",
    "BS = 512\n",
    "ES = 5\n",
    "NUM_FOLDS = 5\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# Paths\n",
    "FOLD_PATH = input_dir / \"folds\"\n",
    "COUNT_PATH = FOLD_PATH / \"batch51_100_counts.csv\"\n",
    "CV_PATH = FOLD_PATH / f\"batch51_100_cv_max_{PULSE_THRESHOLD}_pulses.csv\"\n",
    "WANDB_DIR = log_dir / \"wandb\"\n",
    "PROJECT_NAME = \"icecube\"\n",
    "GROUP_NAME = \"ft_batch_51_100\"\n",
    "\n",
    "CREATE_FOLDS = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"path\": str(DATABASE_PATH),\n",
    "    # \"path\": \"/media/eden/sandisk/projects/icecube/input/sqlite/batch_1.db\",\n",
    "    \"pulsemap\": \"pulse_table\",\n",
    "    \"truth_table\": \"meta_table\",\n",
    "    \"features\": FEATURES.KAGGLE,\n",
    "    \"truth\": TRUTH.KAGGLE,\n",
    "    \"index_column\": \"event_id\",\n",
    "    \"batch_size\": BS,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"target\": \"direction\",\n",
    "    \"run_name_tag\": \"batch_1_50\",\n",
    "    \"early_stopping_patience\": ES,\n",
    "    \"fit\": {\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"gpus\": [0],\n",
    "        \"distribution_strategy\": None,\n",
    "    },\n",
    "    \"base_dir\": \"training\",\n",
    "    \"wandb\": {\n",
    "        \"project\": PROJECT_NAME,\n",
    "        \"group\": GROUP_NAME,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def build_model(config: Dict[str, Any], train_dataloader: Any) -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == \"direction\":\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [\n",
    "            config[\"target\"] + \"_x\",\n",
    "            config[\"target\"] + \"_y\",\n",
    "            config[\"target\"] + \"_z\",\n",
    "            config[\"target\"] + \"_kappa\",\n",
    "        ]\n",
    "        additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=Adam,\n",
    "        optimizer_kwargs={\"lr\": LR, \"eps\": 1e-03},\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "            \"milestones\": [\n",
    "                0,\n",
    "                len(train_dataloader) / 2,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "            ],\n",
    "            \"factors\": [1e-02, 1, 1e-02],\n",
    "        },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_pretrained_model(\n",
    "    config: Dict[str, Any],\n",
    "    state_dict_path: str = \"/kaggle/input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth\",\n",
    ") -> StandardModel:\n",
    "    train_dataloader, _ = make_dataloaders(config=config)\n",
    "    model = build_model(config=config, train_dataloader=train_dataloader)\n",
    "    # model._inference_trainer = Trainer(config['fit'])\n",
    "    model.load_state_dict(state_dict_path)\n",
    "    model.prediction_columns = [\n",
    "        config[\"target\"] + \"_x\",\n",
    "        config[\"target\"] + \"_y\",\n",
    "        config[\"target\"] + \"_z\",\n",
    "        config[\"target\"] + \"_kappa\",\n",
    "    ]\n",
    "    model.additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any], fold: int = 0) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "    df_cv = pd.read_csv(CV_PATH)\n",
    "\n",
    "    val_idx = df_cv[df_cv[\"fold\"] == fold][config[\"index_column\"]].ravel().tolist()\n",
    "    train_idx = (\n",
    "        df_cv[~df_cv[\"fold\"].isin([-1, fold])][config[\"index_column\"]].ravel().tolist()\n",
    "    )\n",
    "\n",
    "    train_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=train_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    validate_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=val_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-19 02:43:05 - SQLiteDataset.warning - Dataset is empty.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = load_pretrained_model(config=config, state_dict_path=\"/media/eden/sandisk/projects/icecube/models/batch1_50/state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(\"/media/eden/sandisk/projects/icecube/models/batch1_50/state_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['_gnn._conv_layers.0.nn.0.weight', '_gnn._conv_layers.0.nn.0.bias', '_gnn._conv_layers.0.nn.2.weight', '_gnn._conv_layers.0.nn.2.bias', '_gnn._conv_layers.1.nn.0.weight', '_gnn._conv_layers.1.nn.0.bias', '_gnn._conv_layers.1.nn.2.weight', '_gnn._conv_layers.1.nn.2.bias', '_gnn._conv_layers.2.nn.0.weight', '_gnn._conv_layers.2.nn.0.bias', '_gnn._conv_layers.2.nn.2.weight', '_gnn._conv_layers.2.nn.2.bias', '_gnn._conv_layers.3.nn.0.weight', '_gnn._conv_layers.3.nn.0.bias', '_gnn._conv_layers.3.nn.2.weight', '_gnn._conv_layers.3.nn.2.bias', '_gnn._post_processing.0.weight', '_gnn._post_processing.0.bias', '_gnn._post_processing.2.weight', '_gnn._post_processing.2.bias', '_gnn._readout.0.weight', '_gnn._readout.0.bias', '_tasks.0._affine.weight', '_tasks.0._affine.bias'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[33mWARNING \u001b[0m 2023-02-19 02:15:39 - SQLiteDataset.warning - Dataset is empty.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_loader, _ = make_dataloaders(config=config)\n",
    "\n",
    "for batch in train_loader:\n",
    "    y = model(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.png'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "make_dot(y[0].mean(), params=dict(model.named_parameters())).render(format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "StandardModel(\n",
    "  (_detector): IceCubeKaggle(\n",
    "    (_graph_builder): KNNGraphBuilder()\n",
    "  )\n",
    "  (_gnn): DynEdge(\n",
    "    (_activation): LeakyReLU(negative_slope=0.01)\n",
    "    (_conv_layers): ModuleList(\n",
    "      (0): DynEdgeConv(nn=Sequential(\n",
    "        (0): Linear(in_features=34, out_features=128, bias=True)\n",
    "        (1): LeakyReLU(negative_slope=0.01)\n",
    "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
    "        (3): LeakyReLU(negative_slope=0.01)\n",
    "      ))\n",
    "      (1): DynEdgeConv(nn=Sequential(\n",
    "        (0): Linear(in_features=512, out_features=336, bias=True)\n",
    "        (1): LeakyReLU(negative_slope=0.01)\n",
    "        (2): Linear(in_features=336, out_features=256, bias=True)\n",
    "        (3): LeakyReLU(negative_slope=0.01)\n",
    "      ))\n",
    "      (2): DynEdgeConv(nn=Sequential(\n",
    "        (0): Linear(in_features=512, out_features=336, bias=True)\n",
    "        (1): LeakyReLU(negative_slope=0.01)\n",
    "        (2): Linear(in_features=336, out_features=256, bias=True)\n",
    "        (3): LeakyReLU(negative_slope=0.01)\n",
    "      ))\n",
    "      (3): DynEdgeConv(nn=Sequential(\n",
    "        (0): Linear(in_features=512, out_features=336, bias=True)\n",
    "        (1): LeakyReLU(negative_slope=0.01)\n",
    "        (2): Linear(in_features=336, out_features=256, bias=True)\n",
    "        (3): LeakyReLU(negative_slope=0.01)\n",
    "      ))\n",
    "    )\n",
    "    (_post_processing): Sequential(\n",
    "      (0): Linear(in_features=1041, out_features=336, bias=True)\n",
    "      (1): LeakyReLU(negative_slope=0.01)\n",
    "      (2): Linear(in_features=336, out_features=256, bias=True)\n",
    "      (3): LeakyReLU(negative_slope=0.01)\n",
    "    )\n",
    "    (_readout): Sequential(\n",
    "      (0): Linear(in_features=768, out_features=128, bias=True)\n",
    "      (1): LeakyReLU(negative_slope=0.01)\n",
    "    )\n",
    "  )\n",
    "  (_tasks): ModuleList(\n",
    "    (0): DirectionReconstructionWithKappa(\n",
    "      (_loss_function): VonMisesFisher3DLoss()\n",
    "      (_affine): Linear(in_features=128, out_features=3, bias=True)\n",
    "    )\n",
    "  )\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icecube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1f5c8a913c9cf5c53f678b20c057b673b014e2f0c3fa6d6f65aab94461faf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
