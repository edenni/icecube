{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-07 03:19:00 - get_logger - Writing log to \u001b[1m/media/eden/sandisk/projects/icecube/logs/graphnet_20230307-031900.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from graphnet.utilities.logging import get_logger\n",
    "from icecube.constants import *\n",
    "\n",
    "logger = get_logger(log_folder=log_dir)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import (EarlyStopping, LearningRateMonitor,\n",
    "                                         ModelCheckpoint)\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.profiler import SimpleProfiler\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import \\\n",
    "    DirectionReconstructionWithKappa\n",
    "from graphnet.training.callbacks import PiecewiseLinearLR, ProgressBar\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss\n",
    "from graphnet.training.utils import make_dataloader\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "PULSEMAP = \"pulse_table\"\n",
    "# DATABASE_PATH = database_dir / \"batch_51_100.db\"\n",
    "DATABASE_PATH = Path(\"/media/eden/sandisk/projects/icecube/input/sqlite/batch_101_150.db\")\n",
    "# DATABASE_PATH = \"/media/eden/sandisk/projects/icecube/input/sqlite/batch_1.db\"\n",
    "PULSE_THRESHOLD = 200\n",
    "SEED = 42\n",
    "\n",
    "# Training configs\n",
    "MAX_EPOCHS = 300\n",
    "LR = 3e-4\n",
    "MOMENTUM = 0.9\n",
    "BS = 512\n",
    "ES = 10\n",
    "NUM_FOLDS = 5\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# Paths\n",
    "FOLD_PATH = input_dir / \"folds\"\n",
    "COUNT_PATH = FOLD_PATH / \"batch51_100_counts.csv\"\n",
    "CV_PATH = FOLD_PATH / f\"batch51_100_cv_max_{PULSE_THRESHOLD}_pulses.csv\"\n",
    "WANDB_DIR = log_dir\n",
    "PROJECT_NAME = \"icecube\"\n",
    "GROUP_NAME = \"resume_batch_101_150_np_200\"\n",
    "\n",
    "CREATE_FOLDS = False\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"path\": str(DATABASE_PATH),\n",
    "    \"pulsemap\": \"pulse_table\",\n",
    "    \"truth_table\": \"meta_table\",\n",
    "    \"features\": FEATURES.KAGGLE,\n",
    "    \"truth\": TRUTH.KAGGLE,\n",
    "    \"index_column\": \"event_id\",\n",
    "    \"batch_size\": BS,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"target\": \"direction\",\n",
    "    \"run_name_tag\": \"batch_1_50\",\n",
    "    \"early_stopping_patience\": ES,\n",
    "    \"fit\": {\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"gpus\": [0],\n",
    "        \"distribution_strategy\": None,\n",
    "        \"limit_train_batches\": 1.0,  # debug\n",
    "        \"limit_val_batches\": 1.0,\n",
    "    },\n",
    "    \"base_dir\": \"training\",\n",
    "    \"wandb\": {\n",
    "        \"project\": PROJECT_NAME,\n",
    "        \"group\": GROUP_NAME,\n",
    "    },\n",
    "    \"lr\": LR,\n",
    "}\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    config: Dict[str, Any], train_dataloader: Any\n",
    ") -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == \"direction\":\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [\n",
    "            config[\"target\"] + \"_x\",\n",
    "            config[\"target\"] + \"_y\",\n",
    "            config[\"target\"] + \"_z\",\n",
    "            config[\"target\"] + \"_kappa\",\n",
    "        ]\n",
    "        additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=SGD,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\": LR,\n",
    "            \"momentum\": MOMENTUM,\n",
    "            \"nesterov\": True,\n",
    "            \"weight_decay\": 1e-4,\n",
    "        },\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "            \"milestones\": [\n",
    "                0,\n",
    "                len(train_dataloader) / 2,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "            ],\n",
    "            \"factors\": [1e-03, 1, 1e-03],\n",
    "        },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_pretrained_model(\n",
    "    config: Dict[str, Any],\n",
    "    state_dict_path: str = \"/kaggle/input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth\",\n",
    ") -> StandardModel:\n",
    "    train_dataloader, _ = make_dataloaders(config=config)\n",
    "    model = build_model(config=config, train_dataloader=train_dataloader)\n",
    "    # model._inference_trainer = Trainer(config['fit'])\n",
    "    state_dict = torch.load(state_dict_path)[\"state_dict\"]\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.prediction_columns = [\n",
    "        config[\"target\"] + \"_x\",\n",
    "        config[\"target\"] + \"_y\",\n",
    "        config[\"target\"] + \"_z\",\n",
    "        config[\"target\"] + \"_kappa\",\n",
    "    ]\n",
    "    model.additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any], fold: int = 0) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "\n",
    "    train_idx = pd.read_csv(\"/media/eden/sandisk/projects/icecube/input/folds/batch101_150_train.csv\")[config[\"index_column\"]].to_list()\n",
    "    val_idx = pd.read_csv(\"/media/eden/sandisk/projects/icecube/input/folds/batch101_150_test.csv\")[config[\"index_column\"]].to_list()\n",
    "\n",
    "    # df_cv = pd.read_csv(CV_PATH)\n",
    "\n",
    "    # val_idx = (\n",
    "    #     df_cv[df_cv[\"fold\"] == fold][config[\"index_column\"]].ravel().tolist()\n",
    "    # )\n",
    "    # train_idx = (\n",
    "    #     df_cv[~df_cv[\"fold\"].isin([-1, fold])][config[\"index_column\"]]\n",
    "    #     .ravel()\n",
    "    #     .tolist()\n",
    "    # )\n",
    "\n",
    "    logger.info(f\"training samples: {len(train_idx)}\")\n",
    "    logger.info(f\"val samples: {len(val_idx)}\")\n",
    "\n",
    "    train_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=train_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    validate_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=val_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "\n",
    "def train_dynedge(\n",
    "    config: Dict[str, Any], fold: int = 0, resume: Path = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Builds(or resumes) and trains GNN according to config.\"\"\"\n",
    "    logger.info(f\"features: {config['features']}\")\n",
    "    logger.info(f\"truth: {config['truth']}\")\n",
    "\n",
    "    # run_name = (\n",
    "    #     f\"dynedge_{config['target']}_{config['run_name_tag']}_fold{fold}\"\n",
    "    # )\n",
    "\n",
    "    run_name = (\n",
    "        f\"dynedge_{config['target']}_{config['run_name_tag']}_np200_aux0.8\"\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=PROJECT_NAME,\n",
    "        group=GROUP_NAME,\n",
    "        name=run_name,\n",
    "        save_dir=WANDB_DIR,\n",
    "        log_model=True,\n",
    "    )\n",
    "    wandb_logger.experiment.config.update(config)\n",
    "\n",
    "    train_dataloader, validate_dataloader = make_dataloaders(\n",
    "        config=config, fold=fold\n",
    "    )\n",
    "\n",
    "    if not resume:\n",
    "        model = build_model(config, train_dataloader)\n",
    "    else:\n",
    "        model = load_pretrained_model(config, state_dict_path=resume)\n",
    "\n",
    "    wandb_logger.experiment.watch(model, log=\"all\")\n",
    "\n",
    "    # Training model\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val/mae\",\n",
    "            patience=config[\"early_stopping_patience\"],\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        ProgressBar(),\n",
    "        ModelCheckpoint(\n",
    "            filename=\"graphnet-{val/mae:.4f}-{epoch:02d}\",\n",
    "            monitor=\"val/mae\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=3,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    profiler = SimpleProfiler(dirpath=log_dir, filename=\"profile\")\n",
    "\n",
    "    model.fit(\n",
    "        train_dataloader,\n",
    "        validate_dataloader,\n",
    "        callbacks=callbacks,\n",
    "        logger=wandb_logger,\n",
    "        profiler=profiler,\n",
    "        **config[\"fit\"],\n",
    "    )\n",
    "\n",
    "    wandb_logger.experiment.save(str(profiler.dirpath / profiler.filename))\n",
    "    wandb_logger.experiment.finish()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_to_3d(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    df[\"true_x\"] = np.cos(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_y\"] = np.sin(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_z\"] = np.cos(df[\"zenith\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_angular_error(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcualtes the opening angle (angular error) between true and reconstructed direction vectors\"\"\"\n",
    "    df[\"angular_error\"] = np.arccos(\n",
    "        df[\"true_x\"] * df[\"direction_x\"]\n",
    "        + df[\"true_y\"] * df[\"direction_y\"]\n",
    "        + df[\"true_z\"] * df[\"direction_z\"]\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-07 03:19:13 - make_dataloaders - training samples: 7023801\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-07 03:19:13 - make_dataloaders - val samples: 780423\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, validate_dataloader = make_dataloaders(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphnet.data.sqlite import SQLiteDataset\n",
    "\n",
    "dataset = SQLiteDataset(\n",
    "    path=\"/media/eden/sandisk/projects/icecube/input/icecube/sqlite/batch_101_150.db\",\n",
    "    pulsemaps=PULSEMAP,\n",
    "    features=config[\"features\"],\n",
    "    truth=config[\"truth\"],\n",
    "    selection=None,\n",
    "    truth_table=\"meta_table\",\n",
    "    index_column=config[\"index_column\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 447 ms, sys: 105 ms, total: 552 ms\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "for idx in range(1024):\n",
    "    dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cv = pd.read_csv(CV_PATH)\n",
    "\n",
    "# val_idx = (\n",
    "#     df_cv[df_cv[\"fold\"] == 0][config[\"index_column\"]].ravel().tolist()\n",
    "# )\n",
    "# val_idx = pd.read_csv(\"/media/eden/sandisk/projects/icecube/input/folds/batch101_150_test.csv\")[config[\"index_column\"]].to_list()\n",
    "\n",
    "dataset = SQLiteDataset(\n",
    "    path=\"/media/eden/sandisk/projects/icecube/input/sqlite/batch_51_100.db\",\n",
    "    pulsemaps=PULSEMAP,\n",
    "    features=config[\"features\"],\n",
    "    truth=config[\"truth\"],\n",
    "    selection=None,\n",
    "    truth_table=\"meta_table\",\n",
    "    index_column=config[\"index_column\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 485 ms, sys: 113 ms, total: 599 ms\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "for idx in range(1024):\n",
    "    dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icecube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1f5c8a913c9cf5c53f678b20c057b673b014e2f0c3fa6d6f65aab94461faf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
