{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning GraphNeT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-07 20:59:33 - get_logger - Writing log to \u001b[1m/media/eden/sandisk/projects/icecube/logs/graphnet_20230307-205933.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "from icecube.constants import *\n",
    "from graphnet.utilities.logging import get_logger\n",
    "\n",
    "logger = get_logger(log_folder=log_dir)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lion_pytorch import Lion\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import Adam, SGD, AdamW, Adagrad\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.data.sqlite.sqlite_dataset import SQLiteDataset\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import (\n",
    "    DirectionReconstructionWithKappa,\n",
    "    ZenithReconstructionWithKappa,\n",
    "    AzimuthReconstructionWithKappa,\n",
    ")\n",
    "from graphnet.training.callbacks import ProgressBar, PiecewiseLinearLR\n",
    "from graphnet.training.loss_functions import (\n",
    "    VonMisesFisher3DLoss,\n",
    "    VonMisesFisher2DLoss,\n",
    ")\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.utils import make_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PULSEMAP = \"pulse_table\"\n",
    "DATABASE_PATH = database_dir / \"batch_51_100.db\"\n",
    "# DATABASE_PATH = \"/media/eden/sandisk/projects/icecube/input/sqlite/batch_1.db\"\n",
    "PULSE_THRESHOLD = 200\n",
    "SEED = 42\n",
    "\n",
    "# Training configs\n",
    "MAX_EPOCHS = 100\n",
    "LR = 0.01\n",
    "BS = 1024\n",
    "ES = 5\n",
    "NUM_FOLDS = 5\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# Paths\n",
    "FOLD_PATH = input_dir / \"folds\"\n",
    "COUNT_PATH = FOLD_PATH / \"batch51_100_counts_auxiliary.csv\"\n",
    "CV_PATH = FOLD_PATH / f\"batch51_100_max_{PULSE_THRESHOLD}_pulses_non_auxiliary.csv\"\n",
    "WANDB_DIR = log_dir\n",
    "PROJECT_NAME = \"icecube\"\n",
    "GROUP_NAME = \"ft_batch_51_100\"\n",
    "\n",
    "CREATE_FOLDS = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_selection(\n",
    "    df: pd.DataFrame, num_folds: int = 5, pulse_threshold: int = 200\n",
    ") -> None:\n",
    "    \"\"\"Creates a validation and training selection (20 - 80). All events in both selections satisfies n_pulses <= 200 by default.\"\"\"\n",
    "    n_events = np.arange(0, len(df), 1)\n",
    "    df[\"fold\"] = 1\n",
    "\n",
    "    # kf = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "    # for i, (_, val_idx) in enumerate(kf.split(n_events)):\n",
    "    #     df.loc[val_idx, \"fold\"] = i\n",
    "\n",
    "    # Remove events with large pulses from training and validation sample (memory)\n",
    "    df[\"fold\"][df[\"n_pulses\"] > pulse_threshold] = -1\n",
    "    df[\"fold\"][df[\"auxiliary\"] == True] = -1\n",
    "\n",
    "    df.to_csv(CV_PATH)\n",
    "    return\n",
    "\n",
    "\n",
    "def get_number_of_pulses(db: Path, event_id: int, pulsemap: str) -> int:\n",
    "    with sqlite3.connect(str(db)) as con:\n",
    "        query = f\"select event_id, auxiliary from {pulsemap} where event_id = {event_id} limit 20000\"\n",
    "        data = con.execute(query).fetchall()\n",
    "    return len(data)\n",
    "\n",
    "\n",
    "def count_pulses(database: Path, pulsemap: str) -> pd.DataFrame:\n",
    "    \"\"\"Will count the number of pulses in each event and return a single dataframe that contains counts for each event_id.\"\"\"\n",
    "    with sqlite3.connect(str(database)) as con:\n",
    "        query = \"select event_id from meta_table\"\n",
    "        events = pd.read_sql(query, con)\n",
    "    counts = {\"event_id\": [], \"n_pulses\": []}\n",
    "\n",
    "    for event_id in tqdm(events[\"event_id\"]):\n",
    "        a = get_number_of_pulses(database, event_id, pulsemap)\n",
    "        counts[\"event_id\"].append(event_id)\n",
    "        counts[\"n_pulses\"].append(a)\n",
    "\n",
    "    df = pd.DataFrame(counts)\n",
    "    df.to_csv(COUNT_PATH)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# r = None\n",
    "\n",
    "# for batch in tqdm(range(51, 100)):\n",
    "#     path = f\"/media/eden/sandisk/projects/icecube/input/icecube/icecube-neutrinos-in-deep-ice/train/batch_{batch}.parquet\"\n",
    "#     df = pl.read_parquet(path)\n",
    "#     df = (df\n",
    "#         .groupby(\"event_id\")\n",
    "#         .agg([\n",
    "#             pl.col(\"auxiliary\").sum().alias(\"aux_sum\"),\n",
    "#             pl.col(\"auxiliary\").count().alias(\"n_pulse\"),\n",
    "#         ])\n",
    "#         .select([\n",
    "#             pl.col(\"event_id\"),\n",
    "#             (pl.col(\"aux_sum\") / pl.col(\"n_pulse\")).alias(\"p_aux\"),\n",
    "#             pl.col(\"n_pulse\"),\n",
    "#         ])\n",
    "#         .filter(\n",
    "#             (pl.col(\"p_aux\") < 0.8) & (pl.col(\"n_pulse\") < 200)\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     r = df if r is None else pl.concat([r, df])\n",
    "\n",
    "\n",
    "# x_train, x_test = train_test_split(r, test_size=0.1)\n",
    "# x_train.write_csv(\"/media/eden/sandisk/projects/icecube/input/folds/batch51_100_train.csv\")\n",
    "# x_test.write_csv(\"/media/eden/sandisk/projects/icecube/input/folds/batch51_100_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CREATE_FOLDS:\n",
    "    df = (\n",
    "        count_pulses(DATABASE_PATH, PULSEMAP)\n",
    "        if not COUNT_PATH.exists()\n",
    "        else pd.read_csv(COUNT_PATH)\n",
    "    )\n",
    "    make_selection(df=df, pulse_threshold=PULSE_THRESHOLD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"path\": str(DATABASE_PATH),\n",
    "    # \"path\": \"/media/eden/sandisk/projects/icecube/input/sqlite/batch_1.db\",\n",
    "    \"pulsemap\": \"pulse_table\",\n",
    "    \"truth_table\": \"meta_table\",\n",
    "    \"features\": FEATURES.KAGGLE,\n",
    "    \"truth\": TRUTH.KAGGLE,\n",
    "    \"index_column\": \"event_id\",\n",
    "    \"batch_size\": BS,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"target\": \"direction\",\n",
    "    \"run_name_tag\": \"batch_51_100\",\n",
    "    \"early_stopping_patience\": ES,\n",
    "    \"fit\": {\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"gpus\": [0],\n",
    "        \"distribution_strategy\": None,\n",
    "        # \"limit_train_batches\": 0.1,  # debug\n",
    "        # \"limit_val_batches\": 0.1,\n",
    "    },\n",
    "    \"base_dir\": \"training\",\n",
    "    \"wandb\": {\n",
    "        \"project\": PROJECT_NAME,\n",
    "        \"group\": GROUP_NAME,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    config: Dict[str, Any], train_dataloader: Any\n",
    ") -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == \"direction\":\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [\n",
    "            config[\"target\"] + \"_x\",\n",
    "            config[\"target\"] + \"_y\",\n",
    "            config[\"target\"] + \"_z\",\n",
    "            config[\"target\"] + \"_kappa\",\n",
    "        ]\n",
    "        additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=SGD,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\": LR,\n",
    "            \"momentum\": 0.9,\n",
    "            \"nesterov\": True,\n",
    "        },\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "            \"milestones\": [\n",
    "                0,\n",
    "                len(train_dataloader) / 2,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"] / 10,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "            ],\n",
    "            \"factors\": [1e-03, 1, 1e-01, 1e-04],\n",
    "        },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_pretrained_model(\n",
    "    config: Dict[str, Any],\n",
    "    state_dict_path: str = \"/kaggle/input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth\",\n",
    ") -> StandardModel:\n",
    "    train_dataloader, _ = make_dataloaders(config=config)\n",
    "    model = build_model(config=config, train_dataloader=train_dataloader)\n",
    "    # model._inference_trainer = Trainer(config['fit'])\n",
    "    model.load_state_dict(state_dict_path)\n",
    "    model.prediction_columns = [\n",
    "        config[\"target\"] + \"_x\",\n",
    "        config[\"target\"] + \"_y\",\n",
    "        config[\"target\"] + \"_z\",\n",
    "        config[\"target\"] + \"_kappa\",\n",
    "    ]\n",
    "    model.additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any], fold: int = 0) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "    df_cv = pd.read_csv(CV_PATH)\n",
    "\n",
    "    val_idx = (\n",
    "        df_cv[df_cv[\"fold\"] == fold][config[\"index_column\"]].ravel().tolist()\n",
    "    )\n",
    "    train_idx = (\n",
    "        df_cv[~df_cv[\"fold\"].isin([-1, fold])][config[\"index_column\"]]\n",
    "        .ravel()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    train_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=train_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    validate_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=val_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "\n",
    "def train_dynedge(\n",
    "    config: Dict[str, Any], fold: int = 0, resume: Path = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Builds(or resumes) and trains GNN according to config.\"\"\"\n",
    "    logger.info(f\"features: {config['features']}\")\n",
    "    logger.info(f\"truth: {config['truth']}\")\n",
    "\n",
    "    run_name = (\n",
    "        f\"dynedge_{config['target']}_{config['run_name_tag']}_fold{fold}\"\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=PROJECT_NAME,\n",
    "        group=GROUP_NAME,\n",
    "        name=run_name,\n",
    "        save_dir=WANDB_DIR,\n",
    "        log_model=True,\n",
    "    )\n",
    "    wandb_logger.experiment.config.update(config)\n",
    "\n",
    "    train_dataloader, validate_dataloader = make_dataloaders(\n",
    "        config=config, fold=fold\n",
    "    )\n",
    "\n",
    "    if not resume:\n",
    "        model = build_model(config, train_dataloader)\n",
    "    else:\n",
    "        model = load_pretrained_model(config, state_dict_path=resume)\n",
    "\n",
    "    # Training model\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_mae\",\n",
    "            patience=config[\"early_stopping_patience\"],\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        ProgressBar(),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataloader,\n",
    "        validate_dataloader,\n",
    "        callbacks=callbacks,\n",
    "        logger=wandb_logger,\n",
    "        **config[\"fit\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_to_3d(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    df[\"true_x\"] = np.cos(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_y\"] = np.sin(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_z\"] = np.cos(df[\"zenith\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_angular_error(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcualtes the opening angle (angular error) between true and reconstructed direction vectors\"\"\"\n",
    "    df[\"angular_error\"] = np.arccos(\n",
    "        df[\"true_x\"] * df[\"direction_x\"]\n",
    "        + df[\"true_y\"] * df[\"direction_y\"]\n",
    "        + df[\"true_z\"] * df[\"direction_z\"]\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, _ = make_dataloaders(config=config)\n",
    "model = build_model(config=config, train_dataloader=train_dataloader)\n",
    "# model._inference_trainer = Trainer(config['fit'])\n",
    "# model.load_state_dict(\"../../models/batch1_50/state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(\"../../models/graphnet_50_100.ckpt\")[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-19 13:44:53 - train_dynedge - features: ['x', 'y', 'z', 'time', 'charge', 'auxiliary']\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-19 13:44:53 - train_dynedge - truth: ['zenith', 'azimuth']\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medenn0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/eden/sandisk/projects/icecube/logs/wandb/run-20230219_134454-dnk61xex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edenn0/icecube/runs/dnk61xex' target=\"_blank\">dynedge_direction_batch_1_50_fold0</a></strong> to <a href='https://wandb.ai/edenn0/icecube' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edenn0/icecube' target=\"_blank\">https://wandb.ai/edenn0/icecube</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edenn0/icecube/runs/dnk61xex' target=\"_blank\">https://wandb.ai/edenn0/icecube/runs/dnk61xex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | _detector | IceCubeKaggle    | 0     \n",
      "1 | _gnn      | DynEdge          | 1.3 M \n",
      "2 | _tasks    | ModuleList       | 387   \n",
      "3 | mae       | MeanAngularError | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.395     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4e6bc4ec6f42ab94872ddd08333d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04108b6c289c47909af20c7b82af014a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "for fold in range(NUM_FOLDS):\n",
    "    train_dynedge(\n",
    "        config=config,\n",
    "        fold=fold,\n",
    "        resume=\"/media/eden/sandisk/projects/icecube/models/batch1_50/state_dict.pth\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-07 21:07:06 - train_dynedge - features: ['x', 'y', 'z', 'time', 'charge', 'auxiliary']\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-07 21:07:06 - train_dynedge - truth: ['zenith', 'azimuth']\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medenn0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/eden/sandisk/projects/icecube/logs/wandb/run-20230307_210709-fw06jaen</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edenn0/icecube/runs/fw06jaen' target=\"_blank\">dynedge_direction_batch_1_50_np200_aux0.8</a></strong> to <a href='https://wandb.ai/edenn0/icecube' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edenn0/icecube' target=\"_blank\">https://wandb.ai/edenn0/icecube</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edenn0/icecube/runs/fw06jaen' target=\"_blank\">https://wandb.ai/edenn0/icecube/runs/fw06jaen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-07 21:07:19 - make_dataloaders - training samples: 6882992\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-07 21:07:19 - make_dataloaders - val samples: 764777\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/pytorch_lightning/profiler/__init__.py:64: LightningDeprecationWarning: `pytorch_lightning.profiler.SimpleProfiler` is deprecated in v1.9.0 and will be removed in v2.0.0. Use the equivalent `pytorch_lightning.profilers.SimpleProfiler` class instead.\n",
      "  \"`pytorch_lightning.profiler.SimpleProfiler` is deprecated in v1.9.0 and will be removed in v2.0.0.\"\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | _detector | IceCubeKaggle    | 0     \n",
      "1 | _gnn      | DynEdge          | 1.3 M \n",
      "2 | _tasks    | ModuleList       | 387   \n",
      "3 | mae       | MeanAngularError | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.395     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921e40893f5e4be4a645839587394e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a769c1e0799940669e75a3486483d115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr-SGD</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>train/loss_step</td><td>▆▄▇▅▂▅▃▃▃▃▁▃█▂▅▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>lr-SGD</td><td>3e-05</td></tr><tr><td>train/loss_step</td><td>2.53565</td></tr><tr><td>trainer/global_step</td><td>15</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dynedge_direction_batch_1_50_np200_aux0.8</strong> at: <a href='https://wandb.ai/edenn0/icecube/runs/fw06jaen' target=\"_blank\">https://wandb.ai/edenn0/icecube/runs/fw06jaen</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>/media/eden/sandisk/projects/icecube/logs/wandb/run-20230307_210709-fw06jaen/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StandardModel(\n",
       "  (_detector): IceCubeKaggle(\n",
       "    (_graph_builder): KNNGraphBuilder()\n",
       "  )\n",
       "  (_gnn): DynEdge(\n",
       "    (_activation): LeakyReLU(negative_slope=0.01)\n",
       "    (_conv_layers): ModuleList(\n",
       "      (0): DynEdgeConv(nn=Sequential(\n",
       "        (0): Linear(in_features=34, out_features=128, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "      ))\n",
       "      (1): DynEdgeConv(nn=Sequential(\n",
       "        (0): Linear(in_features=512, out_features=336, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=336, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "      ))\n",
       "      (2): DynEdgeConv(nn=Sequential(\n",
       "        (0): Linear(in_features=512, out_features=336, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=336, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "      ))\n",
       "      (3): DynEdgeConv(nn=Sequential(\n",
       "        (0): Linear(in_features=512, out_features=336, bias=True)\n",
       "        (1): LeakyReLU(negative_slope=0.01)\n",
       "        (2): Linear(in_features=336, out_features=256, bias=True)\n",
       "        (3): LeakyReLU(negative_slope=0.01)\n",
       "      ))\n",
       "    )\n",
       "    (_post_processing): Sequential(\n",
       "      (0): Linear(in_features=1041, out_features=336, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=336, out_features=256, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (_readout): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (_tasks): ModuleList(\n",
       "    (0): DirectionReconstructionWithKappa(\n",
       "      (_loss_function): VonMisesFisher3DLoss()\n",
       "      (_affine): Linear(in_features=128, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (mae): MeanAngularError()\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from graphnet.utilities.logging import get_logger\n",
    "from icecube.constants import *\n",
    "\n",
    "logger = get_logger(log_folder=log_dir)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import (EarlyStopping, LearningRateMonitor,\n",
    "                                         ModelCheckpoint)\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.profiler import SimpleProfiler\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import SGD\n",
    "from tqdm import tqdm\n",
    "\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import \\\n",
    "    DirectionReconstructionWithKappa\n",
    "from graphnet.training.callbacks import PiecewiseLinearLR, ProgressBar\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.loss_functions import VonMisesFisher3DLoss\n",
    "from graphnet.training.utils import make_dataloader\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "PULSEMAP = \"pulse_table\"\n",
    "# DATABASE_PATH = database_dir / \"batch_51_100.db\"\n",
    "DATABASE_PATH = Path(\"/media/eden/sandisk/projects/icecube/input/sqlite/batch_51_100.db\")\n",
    "# DATABASE_PATH = \"/media/eden/sandisk/projects/icecube/input/sqlite/batch_1.db\"\n",
    "PULSE_THRESHOLD = 200\n",
    "SEED = 42\n",
    "\n",
    "# Training configs\n",
    "MAX_EPOCHS = 100\n",
    "LR = 1e-2\n",
    "MOMENTUM = 0.9\n",
    "BS = 512\n",
    "ES = 10\n",
    "NUM_FOLDS = 5\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# Paths\n",
    "FOLD_PATH = input_dir / \"folds\"\n",
    "COUNT_PATH = FOLD_PATH / \"batch51_100_counts.csv\"\n",
    "CV_PATH = FOLD_PATH / f\"batch51_100_cv_max_{PULSE_THRESHOLD}_pulses.csv\"\n",
    "WANDB_DIR = log_dir\n",
    "PROJECT_NAME = \"icecube\"\n",
    "GROUP_NAME = \"pretrain_batch_51_100_np_200\"\n",
    "\n",
    "CREATE_FOLDS = False\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"path\": str(DATABASE_PATH),\n",
    "    \"pulsemap\": \"pulse_table\",\n",
    "    \"truth_table\": \"meta_table\",\n",
    "    \"features\": FEATURES.KAGGLE,\n",
    "    \"truth\": TRUTH.KAGGLE,\n",
    "    \"index_column\": \"event_id\",\n",
    "    \"batch_size\": BS,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"target\": \"direction\",\n",
    "    \"run_name_tag\": \"batch_1_50\",\n",
    "    \"early_stopping_patience\": ES,\n",
    "    \"fit\": {\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"gpus\": [0],\n",
    "        \"distribution_strategy\": None,\n",
    "        \"limit_train_batches\": 1.0,  # debug\n",
    "        \"limit_val_batches\": 1.0,\n",
    "    },\n",
    "    \"base_dir\": \"training\",\n",
    "    \"wandb\": {\n",
    "        \"project\": PROJECT_NAME,\n",
    "        \"group\": GROUP_NAME,\n",
    "    },\n",
    "    \"lr\": LR,\n",
    "}\n",
    "\n",
    "\n",
    "def build_model(\n",
    "    config: Dict[str, Any], train_dataloader: Any\n",
    ") -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == \"direction\":\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [\n",
    "            config[\"target\"] + \"_x\",\n",
    "            config[\"target\"] + \"_y\",\n",
    "            config[\"target\"] + \"_z\",\n",
    "            config[\"target\"] + \"_kappa\",\n",
    "        ]\n",
    "        additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=SGD,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\": LR,\n",
    "            \"momentum\": MOMENTUM,\n",
    "            \"nesterov\": True,\n",
    "            \"weight_decay\": 1e-4,\n",
    "        },\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "            \"milestones\": [\n",
    "                0,\n",
    "                len(train_dataloader) / 2,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"] / 10,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "            ],\n",
    "            \"factors\": [1e-03, 1, 1e-01, 1e-04],\n",
    "        },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_pretrained_model(\n",
    "    config: Dict[str, Any],\n",
    "    state_dict_path: str = \"/kaggle/input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth\",\n",
    ") -> StandardModel:\n",
    "    train_dataloader, _ = make_dataloaders(config=config)\n",
    "    model = build_model(config=config, train_dataloader=train_dataloader)\n",
    "    # model._inference_trainer = Trainer(config['fit'])\n",
    "    state_dict = torch.load(state_dict_path)[\"state_dict\"]\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.prediction_columns = [\n",
    "        config[\"target\"] + \"_x\",\n",
    "        config[\"target\"] + \"_y\",\n",
    "        config[\"target\"] + \"_z\",\n",
    "        config[\"target\"] + \"_kappa\",\n",
    "    ]\n",
    "    model.additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any], fold: int = 0) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "\n",
    "    train_idx = pd.read_csv(\"/media/eden/sandisk/projects/icecube/input/folds/batch51_100_train.csv\")[config[\"index_column\"]].to_list()\n",
    "    val_idx = pd.read_csv(\"/media/eden/sandisk/projects/icecube/input/folds/batch51_100_test.csv\")[config[\"index_column\"]].to_list()\n",
    "\n",
    "    # df_cv = pd.read_csv(CV_PATH)\n",
    "\n",
    "    # val_idx = (\n",
    "    #     df_cv[df_cv[\"fold\"] == fold][config[\"index_column\"]].ravel().tolist()\n",
    "    # )\n",
    "    # train_idx = (\n",
    "    #     df_cv[~df_cv[\"fold\"].isin([-1, fold])][config[\"index_column\"]]\n",
    "    #     .ravel()\n",
    "    #     .tolist()\n",
    "    # )\n",
    "\n",
    "    logger.info(f\"training samples: {len(train_idx)}\")\n",
    "    logger.info(f\"val samples: {len(val_idx)}\")\n",
    "\n",
    "    train_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=train_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    validate_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=val_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "\n",
    "def train_dynedge(\n",
    "    config: Dict[str, Any], fold: int = 0, resume: Path = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Builds(or resumes) and trains GNN according to config.\"\"\"\n",
    "    logger.info(f\"features: {config['features']}\")\n",
    "    logger.info(f\"truth: {config['truth']}\")\n",
    "\n",
    "    # run_name = (\n",
    "    #     f\"dynedge_{config['target']}_{config['run_name_tag']}_fold{fold}\"\n",
    "    # )\n",
    "\n",
    "    run_name = (\n",
    "        f\"dynedge_{config['target']}_{config['run_name_tag']}_np200_aux0.8\"\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=PROJECT_NAME,\n",
    "        group=GROUP_NAME,\n",
    "        name=run_name,\n",
    "        save_dir=WANDB_DIR,\n",
    "        log_model=True,\n",
    "    )\n",
    "    wandb_logger.experiment.config.update(config)\n",
    "\n",
    "    train_dataloader, validate_dataloader = make_dataloaders(\n",
    "        config=config, fold=fold\n",
    "    )\n",
    "\n",
    "    if not resume:\n",
    "        model = build_model(config, train_dataloader)\n",
    "    else:\n",
    "        model = load_pretrained_model(config, state_dict_path=resume)\n",
    "\n",
    "    wandb_logger.experiment.watch(model, log=\"all\")\n",
    "\n",
    "    # Training model\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val/mae\",\n",
    "            patience=config[\"early_stopping_patience\"],\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        ProgressBar(),\n",
    "        ModelCheckpoint(\n",
    "            filename=\"graphnet-{val/mae:.4f}-{epoch:02d}\",\n",
    "            monitor=\"val/mae\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=3,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    profiler = SimpleProfiler(dirpath=log_dir, filename=\"profile\")\n",
    "\n",
    "    model.fit(\n",
    "        train_dataloader,\n",
    "        validate_dataloader,\n",
    "        callbacks=callbacks,\n",
    "        logger=wandb_logger,\n",
    "        profiler=profiler,\n",
    "        **config[\"fit\"],\n",
    "    )\n",
    "\n",
    "    wandb_logger.experiment.save(str(profiler.dirpath / profiler.filename))\n",
    "    wandb_logger.experiment.finish()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_to_3d(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    df[\"true_x\"] = np.cos(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_y\"] = np.sin(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_z\"] = np.cos(df[\"zenith\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_angular_error(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcualtes the opening angle (angular error) between true and reconstructed direction vectors\"\"\"\n",
    "    df[\"angular_error\"] = np.arccos(\n",
    "        df[\"true_x\"] * df[\"direction_x\"]\n",
    "        + df[\"true_y\"] * df[\"direction_y\"]\n",
    "        + df[\"true_z\"] * df[\"direction_z\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "train_dynedge(\n",
    "    config=config,\n",
    "    # resume=\"/media/eden/sandisk/projects/icecube/models/graphnet/ft-epoch=52-mae=1.0905.ckpt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d1f5c8a913c9cf5c53f678b20c057b673b014e2f0c3fa6d6f65aab94461faf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
