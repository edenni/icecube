{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning GraphNeT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-03-05 23:32:11 - get_logger - Writing log to \u001b[1m/media/eden/sandisk/projects/icecube/logs/graphnet_20230305-233211.log\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "from icecube.constants import *\n",
    "from graphnet.utilities.logging import get_logger\n",
    "\n",
    "logger = get_logger(log_folder=log_dir)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from lion_pytorch import Lion\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.optim import Adam, SGD, AdamW, Adagrad\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from graphnet.data.constants import FEATURES, TRUTH\n",
    "from graphnet.models import StandardModel\n",
    "from graphnet.models.detector.icecube import IceCubeKaggle\n",
    "from graphnet.models.gnn import DynEdge\n",
    "from graphnet.data.sqlite.sqlite_dataset import SQLiteDataset\n",
    "from graphnet.models.graph_builders import KNNGraphBuilder\n",
    "from graphnet.models.task.reconstruction import (\n",
    "    DirectionReconstructionWithKappa,\n",
    "    ZenithReconstructionWithKappa,\n",
    "    AzimuthReconstructionWithKappa,\n",
    ")\n",
    "from graphnet.training.callbacks import ProgressBar, PiecewiseLinearLR\n",
    "from graphnet.training.loss_functions import (\n",
    "    VonMisesFisher3DLoss,\n",
    "    VonMisesFisher2DLoss,\n",
    ")\n",
    "from graphnet.training.labels import Direction\n",
    "from graphnet.training.utils import make_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PULSEMAP = \"pulse_table\"\n",
    "DATABASE_PATH = database_dir / \"batch_51_100.db\"\n",
    "# DATABASE_PATH = \"/media/eden/sandisk/projects/icecube/input/sqlite/batch_1.db\"\n",
    "PULSE_THRESHOLD = 200\n",
    "SEED = 42\n",
    "\n",
    "# Training configs\n",
    "MAX_EPOCHS = 100\n",
    "LR = 5e-3\n",
    "BS = 256\n",
    "ES = 5\n",
    "NUM_FOLDS = 5\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# Paths\n",
    "FOLD_PATH = input_dir / \"folds\"\n",
    "COUNT_PATH = FOLD_PATH / \"batch51_100_counts.csv\"\n",
    "CV_PATH = FOLD_PATH / f\"batch51_100_cv_max_{PULSE_THRESHOLD}_pulses.csv\"\n",
    "WANDB_DIR = log_dir\n",
    "PROJECT_NAME = \"icecube\"\n",
    "GROUP_NAME = \"ft_batch_51_100\"\n",
    "\n",
    "CREATE_FOLDS = True\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_selection(\n",
    "    df: pd.DataFrame, num_folds: int = 5, pulse_threshold: int = 200\n",
    ") -> None:\n",
    "    \"\"\"Creates a validation and training selection (20 - 80). All events in both selections satisfies n_pulses <= 200 by default.\"\"\"\n",
    "    n_events = np.arange(0, len(df), 1)\n",
    "    df[\"fold\"] = 0\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n",
    "    for i, (_, val_idx) in enumerate(kf.split(n_events)):\n",
    "        df.loc[val_idx, \"fold\"] = i\n",
    "\n",
    "    # Remove events with large pulses from training and validation sample (memory)\n",
    "    df[\"fold\"][df[\"n_pulses\"] > pulse_threshold] = -1\n",
    "\n",
    "    df.to_csv(CV_PATH)\n",
    "    return\n",
    "\n",
    "\n",
    "def get_number_of_pulses(db: Path, event_id: int, pulsemap: str) -> int:\n",
    "    with sqlite3.connect(str(db)) as con:\n",
    "        query = f\"select event_id from {pulsemap} where event_id = {event_id} limit 20000\"\n",
    "        data = con.execute(query).fetchall()\n",
    "    return len(data)\n",
    "\n",
    "\n",
    "def count_pulses(database: Path, pulsemap: str) -> pd.DataFrame:\n",
    "    \"\"\"Will count the number of pulses in each event and return a single dataframe that contains counts for each event_id.\"\"\"\n",
    "    with sqlite3.connect(str(database)) as con:\n",
    "        query = \"select event_id from meta_table\"\n",
    "        events = pd.read_sql(query, con)\n",
    "    counts = {\"event_id\": [], \"n_pulses\": []}\n",
    "\n",
    "    for event_id in tqdm(events[\"event_id\"]):\n",
    "        a = get_number_of_pulses(database, event_id, pulsemap)\n",
    "        counts[\"event_id\"].append(event_id)\n",
    "        counts[\"n_pulses\"].append(a)\n",
    "\n",
    "    df = pd.DataFrame(counts)\n",
    "    df.to_csv(COUNT_PATH)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CREATE_FOLDS:\n",
    "    df = (\n",
    "        count_pulses(DATABASE_PATH, PULSEMAP)\n",
    "        if not COUNT_PATH.exists()\n",
    "        else pd.read_csv(COUNT_PATH)\n",
    "    )\n",
    "    make_selection(df=df, pulse_threshold=PULSE_THRESHOLD)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"path\": str(DATABASE_PATH),\n",
    "    # \"path\": \"/media/eden/sandisk/projects/icecube/input/sqlite/batch_1.db\",\n",
    "    \"pulsemap\": \"pulse_table\",\n",
    "    \"truth_table\": \"meta_table\",\n",
    "    \"features\": FEATURES.KAGGLE,\n",
    "    \"truth\": TRUTH.KAGGLE,\n",
    "    \"index_column\": \"event_id\",\n",
    "    \"batch_size\": BS,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"target\": \"direction\",\n",
    "    \"run_name_tag\": \"batch_1_50\",\n",
    "    \"early_stopping_patience\": ES,\n",
    "    \"fit\": {\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"gpus\": [0],\n",
    "        \"distribution_strategy\": None,\n",
    "        # \"limit_train_batches\": 10,  # debug\n",
    "        # \"limit_val_batches\": 10,\n",
    "    },\n",
    "    \"base_dir\": \"training\",\n",
    "    \"wandb\": {\n",
    "        \"project\": PROJECT_NAME,\n",
    "        \"group\": GROUP_NAME,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "err = torch.tensor([torch.nan, torch.inf])\n",
    "\n",
    "torch.clip(err, 0, 2*torch.pi).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f9d74332b00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1466, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eden/anaconda3/envs/icecube/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1397, in _shutdown_workers\n",
      "    if not self._shutdown:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_shutdown'\n"
     ]
    }
   ],
   "source": [
    "def build_model(\n",
    "    config: Dict[str, Any], train_dataloader: Any\n",
    ") -> StandardModel:\n",
    "    \"\"\"Builds GNN from config\"\"\"\n",
    "    # Building model\n",
    "    detector = IceCubeKaggle(\n",
    "        graph_builder=KNNGraphBuilder(nb_nearest_neighbours=8),\n",
    "    )\n",
    "    gnn = DynEdge(\n",
    "        nb_inputs=detector.nb_outputs,\n",
    "        global_pooling_schemes=[\"min\", \"max\", \"mean\"],\n",
    "    )\n",
    "\n",
    "    if config[\"target\"] == \"direction\":\n",
    "        task = DirectionReconstructionWithKappa(\n",
    "            hidden_size=gnn.nb_outputs,\n",
    "            target_labels=config[\"target\"],\n",
    "            loss_function=VonMisesFisher3DLoss(),\n",
    "        )\n",
    "        prediction_columns = [\n",
    "            config[\"target\"] + \"_x\",\n",
    "            config[\"target\"] + \"_y\",\n",
    "            config[\"target\"] + \"_z\",\n",
    "            config[\"target\"] + \"_kappa\",\n",
    "        ]\n",
    "        additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "\n",
    "    model = StandardModel(\n",
    "        detector=detector,\n",
    "        gnn=gnn,\n",
    "        tasks=[task],\n",
    "        optimizer_class=SGD,\n",
    "        optimizer_kwargs={\n",
    "            \"lr\": LR,\n",
    "            \"momentum\": 0.9,\n",
    "            \"nesterov\": True,\n",
    "        },\n",
    "        scheduler_class=PiecewiseLinearLR,\n",
    "        scheduler_kwargs={\n",
    "            \"milestones\": [\n",
    "                0,\n",
    "                len(train_dataloader) / 2,\n",
    "                len(train_dataloader) * config[\"fit\"][\"max_epochs\"],\n",
    "            ],\n",
    "            \"factors\": [1e-03, 1, 1e-03],\n",
    "        },\n",
    "        scheduler_config={\n",
    "            \"interval\": \"step\",\n",
    "        },\n",
    "    )\n",
    "    model.prediction_columns = prediction_columns\n",
    "    model.additional_attributes = additional_attributes\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_pretrained_model(\n",
    "    config: Dict[str, Any],\n",
    "    state_dict_path: str = \"/kaggle/input/dynedge-pretrained/dynedge_pretrained_batch_1_to_50/state_dict.pth\",\n",
    ") -> StandardModel:\n",
    "    train_dataloader, _ = make_dataloaders(config=config)\n",
    "    model = build_model(config=config, train_dataloader=train_dataloader)\n",
    "    # model._inference_trainer = Trainer(config['fit'])\n",
    "    model.load_state_dict(state_dict_path)\n",
    "    model.prediction_columns = [\n",
    "        config[\"target\"] + \"_x\",\n",
    "        config[\"target\"] + \"_y\",\n",
    "        config[\"target\"] + \"_z\",\n",
    "        config[\"target\"] + \"_kappa\",\n",
    "    ]\n",
    "    model.additional_attributes = [\"zenith\", \"azimuth\", \"event_id\"]\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_dataloaders(config: Dict[str, Any], fold: int = 0) -> List[Any]:\n",
    "    \"\"\"Constructs training and validation dataloaders for training with early stopping.\"\"\"\n",
    "    df_cv = pd.read_csv(CV_PATH)\n",
    "\n",
    "    val_idx = (\n",
    "        df_cv[df_cv[\"fold\"] == fold][config[\"index_column\"]].ravel().tolist()\n",
    "    )\n",
    "    train_idx = (\n",
    "        df_cv[~df_cv[\"fold\"].isin([-1, fold])][config[\"index_column\"]]\n",
    "        .ravel()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    train_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=train_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=True,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    validate_dataloader = make_dataloader(\n",
    "        db=config[\"path\"],\n",
    "        selection=val_idx,\n",
    "        pulsemaps=config[\"pulsemap\"],\n",
    "        features=FEATURES.KAGGLE,\n",
    "        truth=TRUTH.KAGGLE,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        num_workers=config[\"num_workers\"],\n",
    "        shuffle=False,\n",
    "        labels={\"direction\": Direction()},\n",
    "        index_column=config[\"index_column\"],\n",
    "        truth_table=config[\"truth_table\"],\n",
    "    )\n",
    "\n",
    "    return train_dataloader, validate_dataloader\n",
    "\n",
    "\n",
    "def train_dynedge(\n",
    "    config: Dict[str, Any], fold: int = 0, resume: Path = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Builds(or resumes) and trains GNN according to config.\"\"\"\n",
    "    logger.info(f\"features: {config['features']}\")\n",
    "    logger.info(f\"truth: {config['truth']}\")\n",
    "\n",
    "    run_name = (\n",
    "        f\"dynedge_{config['target']}_{config['run_name_tag']}_fold{fold}\"\n",
    "    )\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=PROJECT_NAME,\n",
    "        group=GROUP_NAME,\n",
    "        name=run_name,\n",
    "        save_dir=WANDB_DIR,\n",
    "        log_model=True,\n",
    "    )\n",
    "    wandb_logger.experiment.config.update(config)\n",
    "\n",
    "    train_dataloader, validate_dataloader = make_dataloaders(\n",
    "        config=config, fold=fold\n",
    "    )\n",
    "\n",
    "    if not resume:\n",
    "        model = build_model(config, train_dataloader)\n",
    "    else:\n",
    "        model = load_pretrained_model(config, state_dict_path=resume)\n",
    "\n",
    "    # Training model\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor=\"val_mae\",\n",
    "            patience=config[\"early_stopping_patience\"],\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "        ProgressBar(),\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        train_dataloader,\n",
    "        validate_dataloader,\n",
    "        callbacks=callbacks,\n",
    "        logger=wandb_logger,\n",
    "        **config[\"fit\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def convert_to_3d(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Converts zenith and azimuth to 3D direction vectors\"\"\"\n",
    "    df[\"true_x\"] = np.cos(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_y\"] = np.sin(df[\"azimuth\"]) * np.sin(df[\"zenith\"])\n",
    "    df[\"true_z\"] = np.cos(df[\"zenith\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_angular_error(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calcualtes the opening angle (angular error) between true and reconstructed direction vectors\"\"\"\n",
    "    df[\"angular_error\"] = np.arccos(\n",
    "        df[\"true_x\"] * df[\"direction_x\"]\n",
    "        + df[\"true_y\"] * df[\"direction_y\"]\n",
    "        + df[\"true_z\"] * df[\"direction_z\"]\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, _ = make_dataloaders(config=config)\n",
    "model = build_model(config=config, train_dataloader=train_dataloader)\n",
    "# model._inference_trainer = Trainer(config['fit'])\n",
    "# model.load_state_dict(\"../../models/batch1_50/state_dict.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "state_dict = torch.load(\"../../models/graphnet_50_100.ckpt\")[\"state_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-19 13:44:53 - train_dynedge - features: ['x', 'y', 'z', 'time', 'charge', 'auxiliary']\u001b[0m\n",
      "\u001b[1;34mgraphnet\u001b[0m: \u001b[32mINFO    \u001b[0m 2023-02-19 13:44:53 - train_dynedge - truth: ['zenith', 'azimuth']\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medenn0\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/eden/sandisk/projects/icecube/logs/wandb/run-20230219_134454-dnk61xex</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edenn0/icecube/runs/dnk61xex' target=\"_blank\">dynedge_direction_batch_1_50_fold0</a></strong> to <a href='https://wandb.ai/edenn0/icecube' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edenn0/icecube' target=\"_blank\">https://wandb.ai/edenn0/icecube</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edenn0/icecube/runs/dnk61xex' target=\"_blank\">https://wandb.ai/edenn0/icecube/runs/dnk61xex</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | _detector | IceCubeKaggle    | 0     \n",
      "1 | _gnn      | DynEdge          | 1.3 M \n",
      "2 | _tasks    | ModuleList       | 387   \n",
      "3 | mae       | MeanAngularError | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.395     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4e6bc4ec6f42ab94872ddd08333d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04108b6c289c47909af20c7b82af014a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "for fold in range(NUM_FOLDS):\n",
    "    train_dynedge(\n",
    "        config=config,\n",
    "        fold=fold,\n",
    "        resume=\"/media/eden/sandisk/projects/icecube/models/batch1_50/state_dict.pth\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icecube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1f5c8a913c9cf5c53f678b20c057b673b014e2f0c3fa6d6f65aab94461faf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
