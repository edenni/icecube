# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: event.yaml
  - override /model: lstm.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml
  - override /logger: wandb.yaml
  - override /model/scheduler: coswarmup.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

hydra:
  job:
    name: pointpicker_lstm

trainer:
  max_epochs: 100

callbacks:
  early_stopping:
    patience: 10
    min_delta: 0.002
  model_checkpoint:
    filename: "lstm-{val/mae:.3f}-{epoch:03d}"

data:
  batch_size: 4096
  shift_azimuth: false

model:
  net_name: lstm
  num_layers: 3
  bidirectional: true
  bias: false

  optimizer:
    lr: 5e-4
  scheduler:
    first_cycle_steps: 100
    max_lr: 8e-3
    min_lr: 1e-7
    warmup_steps: 5
  scheduler_conf:
    interval: epoch

logger:
  project: icecube
  name: pointpicker_lstm_batch_100_190_bi3l
  group: lstm
  log_model: true

num_bins: 24

# train
train: true
cv: null
fold: null